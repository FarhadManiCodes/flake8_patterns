name: CI

on:
  push:
    branches: [ main, develop, master ]
  pull_request:
    branches: [ main, master ]
  workflow_dispatch:  # Allow manual triggering

# Concurrency control
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: ["3.11", "3.12", "3.13"]
        # Modern Python versions with native union syntax support

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}

    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ matrix.python-version }}-${{ hashFiles('**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-${{ matrix.python-version }}-
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,test]"

    - name: Run pre-commit
      run: |
        pre-commit run --all-files

    - name: Run basic functionality tests
      run: |
        # TODO: Re-enable pytest once EFP105 (first rule) is implemented
        # pytest --cov=flake8_patterns --cov-report=xml --cov-report=html --cov-report=term-missing
        echo "Running basic CI tests (pytest will be enabled once rules are implemented)..."
        python test_ci_basic.py

    # TODO: Re-enable when pytest import issues are resolved (after rule implementation)
    # - name: Upload coverage to Codecov
    #   uses: codecov/codecov-action@v4
    #   with:
    #     file: ./coverage.xml
    #     flags: unittests
    #     name: codecov-umbrella
    #     fail_ci_if_error: false

    - name: Run type checking
      run: |
        mypy src/

    - name: Test plugin installation
      run: |
        python scripts/verify_installation.py

    - name: Test plugin functionality
      run: |
        flake8 --select=EFP examples/bad_patterns.py || true
        flake8 examples/good_patterns.py

    # TODO: Re-enable when pytest coverage is working
    # - name: Upload test artifacts
    #   uses: actions/upload-artifact@v4
    #   if: always()
    #   with:
    #     name: test-results-${{ matrix.python-version }}
    #     path: |
    #       htmlcov/
    #       coverage.xml
    #       .coverage

  security:
    runs-on: ubuntu-latest
    needs: test

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.12"

    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-security-${{ hashFiles('**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-security-
          ${{ runner.os }}-pip-

    - name: Install security tools
      run: |
        python -m pip install --upgrade pip
        pip install bandit[toml]

    - name: Run bandit security scan
      run: |
        bandit -r src/ -f json -o bandit-report.json || true
        bandit -r src/ -f txt

    - name: Upload security artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: security-reports
        path: |
          bandit-report.json

  integration:
    runs-on: ubuntu-latest
    needs: test
    strategy:
      fail-fast: false
      matrix:
        flake8-version: ["7.0.0", "7.1.0", "latest"]
        include:
          - flake8-version: "latest"
            python-version: "3.13"
          - flake8-version: "7.0.0"
            python-version: "3.11"
          - flake8-version: "7.1.0"
            python-version: "3.12"

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version || '3.12' }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version || '3.12' }}

    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-integration-${{ matrix.flake8-version }}-${{ hashFiles('**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-integration-
          ${{ runner.os }}-pip-

    - name: Install flake8 and plugin
      run: |
        python -m pip install --upgrade pip
        if [ "${{ matrix.flake8-version }}" = "latest" ]; then
          pip install flake8
        else
          pip install flake8==${{ matrix.flake8-version }}
        fi
        pip install -e ".[test]"

    - name: Test plugin compatibility
      run: |
        echo "Testing with flake8 $(flake8 --version)"
        flake8 --version | grep flake8_patterns || echo "Plugin not visible in version output"

        # Test plugin works with this flake8 version
        python scripts/verify_installation.py

    - name: Test with other plugins
      run: |
        # Install other popular flake8 plugins
        pip install flake8-bugbear flake8-comprehensions flake8-simplify

        # Test no conflicts
        flake8 --version
        echo "Testing plugin compatibility..."
        flake8 --select=EFP,B,C examples/bad_patterns.py || echo "Some violations expected"

        # Test good patterns don't trigger our rules
        flake8 --select=EFP examples/good_patterns.py

  performance:
    runs-on: ubuntu-latest
    needs: test

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.12"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install pytest-benchmark

    - name: Run performance benchmarks
      run: |
        # Create a benchmark script
        cat > benchmark_test.py << 'EOF'
        import pytest
        import ast
        from flake8_patterns.checker import PatternChecker

        # Simple benchmark test
        def test_benchmark_plugin_overhead(benchmark):
            code = '''
        def test_function():
            data = [1, 2, 3, 4, 5]
            first = data[0]
            second = data[1]
            return first + second
        '''
            tree = ast.parse(code)

            def run_checker():
                checker = PatternChecker(tree)
                return list(checker.run())

            result = benchmark(run_checker)
            assert isinstance(result, list)
        EOF

        pytest benchmark_test.py --benchmark-only --benchmark-json=benchmark-results.json || true

    - name: Upload performance artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-results
        path: |
          benchmark-results.json
